**Recent updates**
- Submitted jobs to complete the filtering step, but they kept getting killed 
  - I submitted 200G last 
  - ```sacct -j 60509221 --format=ReqMem``` shows how much memory was actually alloted to the job 
  - SOLUTION: needed to out ```--mem=XXG``` BEFORE the script name 
  - script worked with ```sbatch --mem=16G run_postprocessing_kfAML.sh```
- It completed 

**Next Steps**

- [x] To complete the filtering step
- [ ] (if there's time) create a pie chart to visualize all of the splicing events 
- [x] subset the data into high and low risk groups from the modified check metadata script --> ```csv_indices_from_bams.py```
    output: ![image](https://user-images.githubusercontent.com/54278292/184235326-76f62472-611a-4e24-a6a6-bebabf8f92e2.png)

- [ ] run the stats mode on the high and low risk groups 
